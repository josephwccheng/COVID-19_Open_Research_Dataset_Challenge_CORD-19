{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # 3. Create a search engine using TFIDF\n",
    "0. Import libraries and dataset\n",
    "1. data preprocess labelled data\n",
    "3. Create TFIDF vectoriser from literatures that are included in the dataset\n",
    "4. Cosine Similarity\n",
    "5. Evaluation of the search engine using the labelled data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.0. Import Libraries and Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\josep\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\josep\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import all the required Library\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "# Text preprocessing libraries\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "nltk.download('wordnet') \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "# libraries for keyword extraction with tf-idf\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the pickle files created from previous notebooks\n",
    "scoped_categorised_literature = pd.read_pickle(\"./1_scoped_cat_lit.pkl\")\n",
    "extracted_literature_data = pd.read_pickle(\"./2_extracted_literature_data.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['extract_id', 'json_path', 'section', 'text'], dtype='object')"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extracted_literature_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Date', 'question_idx', 'pdf_json_files', 'pmc_json_files', 'Study',\n",
       "       'Study Link', 'Journal', 'Study Type', 'Factors', 'Influential',\n",
       "       'Excerpt', 'Measure of Evidence', 'Added on'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scoped_categorised_literature.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Creating function for text preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'doesn', 'what', 'in', 'but', 'having', 'of', 'yourselves', 'our', 'because', 'and', 'me', 'up', \"shan't\", 'own', 'needn', \"wouldn't\", 'i', 'd', \"shouldn't\", 'was', 'ourselves', \"couldn't\", 'wasn', 'by', 'itself', 'are', 'why', 'hadn', 'between', 'my', 'most', 'under', 'about', 'or', \"didn't\", 'which', 'same', 'no', 'only', 'herself', 'll', 'm', 'then', 'where', 'myself', 'do', 'other', 'further', 'below', 'against', \"you'd\", 'her', 'these', 'until', 'off', 'each', \"you're\", 'few', 'being', 'above', 'it', 'am', 'does', 'has', 'who', 'you', 've', 'once', \"mightn't\", 'through', \"it's\", 'don', 'ain', 're', 'been', 'himself', \"you've\", 'them', 'again', 'ours', 'him', 'than', 'whom', \"don't\", 'all', \"doesn't\", 'isn', 'any', 'during', 'after', 'this', 'some', \"you'll\", 'down', 'for', \"that'll\", \"hasn't\", \"isn't\", 'its', 'is', 'how', 'with', 'from', 'be', 'that', 'out', 'his', 'shouldn', 'while', 'can', 'we', 'hers', \"wasn't\", 'should', 'o', 't', \"haven't\", 'themselves', 'had', 'couldn', 'he', 'aren', 'not', 'haven', 'won', 'mightn', 'over', 'their', 'yours', 'such', 'ma', 'have', 'before', \"mustn't\", \"weren't\", 'as', 'more', 'hasn', \"won't\", 'so', 'into', \"aren't\", 'will', 'here', 's', 'an', 'the', 'just', \"she's\", 'mustn', 'when', 'theirs', 'both', 'nor', 'to', \"should've\", 'didn', 'wouldn', 'were', 'on', 'y', 'did', 'very', 'yourself', 'if', 'a', 'your', 'those', 'there', 'now', 'too', 'at', 'weren', \"hadn't\", 'she', 'doing', 'shan', 'they', \"needn't\"}\n"
     ]
    }
   ],
   "source": [
    "# printing all the stop words\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(inputText):\n",
    "    #define stopwords\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    #lower case the text\n",
    "    outputText = inputText.lower()\n",
    "    #Convrt percentages into the string percent\n",
    "    outputText = re.sub('(\\\\d+%)', 'percent', outputText)\n",
    "    # Remove special characters and digits\n",
    "    outputText=re.sub(\"(\\\\d|\\\\W)+\",\" \",outputText)    \n",
    "    # Tokenisation\n",
    "    outputText = outputText.split()\n",
    "    # Remove Stop Words\n",
    "    outputText = [word for word in outputText if not word in stop_words]\n",
    "    # Stemming\n",
    "    ps=PorterStemmer()\n",
    "    outputText = [ps.stem(word) for word in outputText]\n",
    "    # Lemmatisation\n",
    "    lem = WordNetLemmatizer()\n",
    "    outputText = [lem.lemmatize(word) for word in outputText] \n",
    "    outputText = \" \".join(outputText) \n",
    "    \n",
    "    return outputTex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1. Testing the text pre-processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Figure 10 shows that the number of the exposed individuals in region1 decreases from 868.52 (without controls) to 482.05 (with controls) at the end of the implementation of the proposed strategy. Figure 11 demonstrates that the number of the infected individuals in region 1 decreases from 657.01 (without controls) to 364.95 (with controls) at the end of the implementation of the proposed strategy. Also, the number of the quarantined individuals increases significantly from 10.15 (without controls) to 224.57 (with controls).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'figur show number expo individu region decreas without control control end implement propos strategi figur demonstr number infect individu region decreas without control control end implement propos strategi also number quarantin individu increas significantli without control control'"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing the preprocessing\n",
    "text = scoped_categorised_literature.iloc[3]['Excerpt']\n",
    "print(text)\n",
    "preprocess(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2. Generate TF-IDF Vector space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying data preprocessing to all the text we've extracted from the JSON file\n",
    "extracted_literature_data['text'] = extracted_literature_data['text'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       covid declar pandem date covid affect peopl wo...\n",
      "1       sever acut respiratori syndrom coronaviru sar ...\n",
      "2       label pandem covid affect peopl worldwid major...\n",
      "3       facilit characteri sar cov comparison made bet...\n",
      "4       studi look first confirm case ncip provid evid...\n",
      "                              ...                        \n",
      "6491    studi period henc time seri length longer hube...\n",
      "6492    conclu meteorolog factor influenc covid transm...\n",
      "6493    declar conflict interest certifi peer review a...\n",
      "6494    certifi peer review author funder grant medrxi...\n",
      "6495    copyright holder preprint version post march h...\n",
      "Name: text, Length: 6496, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(extracted_literature_data['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tfidfVectorizer object\n",
    "vectorizer = TfidfVectorizer()\n",
    "#fit_transform method to convert given text into TF-IDF scores for all the documents\n",
    "tfidf_transform = vectorizer.fit_transform(extracted_literature_data['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Cosine Similarity "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.1. Testing a user query on running the TFIDF search engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after preprocessing:  breath difficulti give oxygem therapi\n"
     ]
    }
   ],
   "source": [
    "# Create a test example of how to run the search engine\n",
    "query = 'breathing difficulty give oxygem therapy'\n",
    "query = preprocess(query)\n",
    "query_vec = vectorizer.transform([query])\n",
    "print(query_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________\n",
      "[  34 6021  244   31   33]\n",
      "[0.158, 0.162, 0.197, 0.259, 0.382]\n"
     ]
    }
   ],
   "source": [
    "# Using cosine_similarity to get cosine similarities for a query vs all the document available in the text\n",
    "result = cosine_similarity(tfidf_transform, query_vec)\n",
    "result = [i[0] for i in result]\n",
    "\n",
    "# obtaining the top 5 vaules and print the name\n",
    "N = 5\n",
    "top_5_idx = np.argsort(result)[-N:]\n",
    "top_5_rating = []\n",
    "\n",
    "print(top_5_idx)\n",
    "for i in top_5_idx:\n",
    "    top_5_rating.append(round(result[i],3))\n",
    "print(top_5_rating)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4. Evaluation of the search engine using the labelled data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
