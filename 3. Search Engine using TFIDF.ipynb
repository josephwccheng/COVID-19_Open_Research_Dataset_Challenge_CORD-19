{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # 3. Create a search engine using TFIDF\n",
    "0. Import libraries and dataset\n",
    "1. data preprocess labelled data\n",
    "3. Create TFIDF vectoriser from literatures that are included in the dataset\n",
    "4\n",
    "5. Evaluation of Linear Support Vector Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\josep\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\josep\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import all the required Library\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "# Text preprocessing libraries\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "nltk.download('wordnet') \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "# libraries for keyword extraction with tf-idf\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the pickle files created from previous notebooks\n",
    "scoped_categorised_literature = pd.read_pickle(\"./1_scoped_cat_lit.pkl\")\n",
    "extracted_literature_data = pd.read_pickle(\"./2_extracted_literature_data.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['extract_id', 'json_path', 'section', 'text'], dtype='object')"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extracted_literature_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Date', 'question_idx', 'pdf_json_files', 'pmc_json_files', 'Study',\n",
       "       'Study Link', 'Journal', 'Study Type', 'Factors', 'Influential',\n",
       "       'Excerpt', 'Measure of Evidence', 'Added on'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scoped_categorised_literature.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1. Creating function for text preprocessing\n",
    "1. remove all the stopwords\n",
    "2. remove punctuations\n",
    "3. covert to lower\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'doesn', 'what', 'in', 'but', 'having', 'of', 'yourselves', 'our', 'because', 'and', 'me', 'up', \"shan't\", 'own', 'needn', \"wouldn't\", 'i', 'd', \"shouldn't\", 'was', 'ourselves', \"couldn't\", 'wasn', 'by', 'itself', 'are', 'why', 'hadn', 'between', 'my', 'most', 'under', 'about', 'or', \"didn't\", 'which', 'same', 'no', 'only', 'herself', 'll', 'm', 'then', 'where', 'myself', 'do', 'other', 'further', 'below', 'against', \"you'd\", 'her', 'these', 'until', 'off', 'each', \"you're\", 'few', 'being', 'above', 'it', 'am', 'does', 'has', 'who', 'you', 've', 'once', \"mightn't\", 'through', \"it's\", 'don', 'ain', 're', 'been', 'himself', \"you've\", 'them', 'again', 'ours', 'him', 'than', 'whom', \"don't\", 'all', \"doesn't\", 'isn', 'any', 'during', 'after', 'this', 'some', \"you'll\", 'down', 'for', \"that'll\", \"hasn't\", \"isn't\", 'its', 'is', 'how', 'with', 'from', 'be', 'that', 'out', 'his', 'shouldn', 'while', 'can', 'we', 'hers', \"wasn't\", 'should', 'o', 't', \"haven't\", 'themselves', 'had', 'couldn', 'he', 'aren', 'not', 'haven', 'won', 'mightn', 'over', 'their', 'yours', 'such', 'ma', 'have', 'before', \"mustn't\", \"weren't\", 'as', 'more', 'hasn', \"won't\", 'so', 'into', \"aren't\", 'will', 'here', 's', 'an', 'the', 'just', \"she's\", 'mustn', 'when', 'theirs', 'both', 'nor', 'to', \"should've\", 'didn', 'wouldn', 'were', 'on', 'y', 'did', 'very', 'yourself', 'if', 'a', 'your', 'those', 'there', 'now', 'too', 'at', 'weren', \"hadn't\", 'she', 'doing', 'shan', 'they', \"needn't\"}\n"
     ]
    }
   ],
   "source": [
    "# printing all the stop words\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(inputText):\n",
    "    #define stopwords\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    #lower case the text\n",
    "    outputText = inputText.lower()\n",
    "    #Convrt percentages into the string percent\n",
    "    outputText = re.sub('(\\\\d+%)', 'percent', outputText)\n",
    "    # Remove special characters and digits\n",
    "    outputText=re.sub(\"(\\\\d|\\\\W)+\",\" \",outputText)    \n",
    "    # Tokenisation\n",
    "    outputText = outputText.split()\n",
    "    # Remove Stop Words\n",
    "    outputText = [word for word in outputText if not word in stop_words]\n",
    "    # Stemming\n",
    "    ps=PorterStemmer()\n",
    "    outputText = [ps.stem(word) for word in outputText]\n",
    "    # Lemmatisation\n",
    "    lem = WordNetLemmatizer()\n",
    "    outputText = [lem.lemmatize(word) for word in outputText] \n",
    "    outputText = \" \".join(outputText) \n",
    "    \n",
    "    return outputTex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Figure 10 shows that the number of the exposed individuals in region1 decreases from 868.52 (without controls) to 482.05 (with controls) at the end of the implementation of the proposed strategy. Figure 11 demonstrates that the number of the infected individuals in region 1 decreases from 657.01 (without controls) to 364.95 (with controls) at the end of the implementation of the proposed strategy. Also, the number of the quarantined individuals increases significantly from 10.15 (without controls) to 224.57 (with controls).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'figur show number expo individu region decreas without control control end implement propos strategi figur demonstr number infect individu region decreas without control control end implement propos strategi also number quarantin individu increas significantli without control control'"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing the preprocessing\n",
    "text = scoped_categorised_literature.iloc[3]['Excerpt']\n",
    "print(text)\n",
    "preprocess(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2. Generate TF-IDF using Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying data preprocessing to all the text we've extracted from the JSON file\n",
    "extracted_literature_data['text'] = extracted_literature_data['text'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       covid declar pandem date covid affect peopl wo...\n",
      "1       sever acut respiratori syndrom coronaviru sar ...\n",
      "2       label pandem covid affect peopl worldwid major...\n",
      "3       facilit characteri sar cov comparison made bet...\n",
      "4       studi look first confirm case ncip provid evid...\n",
      "                              ...                        \n",
      "6491    studi period henc time seri length longer hube...\n",
      "6492    conclu meteorolog factor influenc covid transm...\n",
      "6493    declar conflict interest certifi peer review a...\n",
      "6494    certifi peer review author funder grant medrxi...\n",
      "6495    copyright holder preprint version post march h...\n",
      "Name: text, Length: 6496, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(extracted_literature_data['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tfidfVectorizer object\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "#fit_transform method to convert given text into TF-IDF scores for all the documents\n",
    "tfidf_transform = vectorizer.fit_transform(extracted_literature_data['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after preprocessing:  breath difficulti give oxygem therapi\n"
     ]
    }
   ],
   "source": [
    "# Get the TF-IDF vector representation of the query\n",
    "# query = extracted_literature_data.iloc[33]['text']\n",
    "query = 'breathing difficulty give oxygem therapy'\n",
    "# print(\"before preprocessing: \", query)\n",
    "query = preprocess(query)\n",
    "print(\"after preprocessing: \", query)\n",
    "query_vec = vectorizer.transform([query])\n",
    "# print(query_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________\n",
      "[  34 6021  244   31   33]\n",
      "[0.158, 0.162, 0.197, 0.259, 0.382]\n"
     ]
    }
   ],
   "source": [
    "# Using cosine_similarity to get cosine similarities for a query vs all the document available in the text\n",
    "result = cosine_similarity(tfidf_transform, query_vec)\n",
    "result = [i[0] for i in result]\n",
    "# obtaining the top 5 vaules and print the name\n",
    "top_5_idx = np.argsort(result)[-5:]\n",
    "top_5_rating = []\n",
    "\n",
    "print(\"____________\")\n",
    "print(top_5_idx)\n",
    "for i in top_5_idx:\n",
    "    top_5_rating.append(round(result[i],3))\n",
    "\n",
    "print(top_5_rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7600\n"
     ]
    }
   ],
   "source": [
    "# Creating Vocabulary dictionary\n",
    "# Vocab is the list of all possible words in the corpus\n",
    "vocabulary = set()\n",
    "\n",
    "for doc in extracted_literature_data['text']:\n",
    "    vocabulary.update(doc.split(','))\n",
    "\n",
    "vocabulary = list(vocabulary)\n",
    "print(len(vocabulary))\n",
    "#Initialising the TFIDF model\n",
    "# tfidf = TfidfVectorizer(vocabulary=vocabulary)\n",
    "\n",
    "# # Fitting the TfIdf model\n",
    "# tfidf.fit(extracted_literature_data['text'])\n",
    "\n",
    "# # Transform the TfIdf model\n",
    "# transformed_tfidf = tfidf.transform(extracted_literature_data['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Term Frequency\n",
    "# frequency of a word in a single document\n",
    "# highly depending on the lengh of the document -> i.e. more words means more frequent\n",
    "# therefore we normalise the frequency value by the total lengh of the document\n",
    "# tf(word, document) = count of word in document / total number in document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document Frequency\n",
    "# number of ducments in which the word is present\n",
    "# measure the importance of ducument in whole set of corpus\n",
    "# count of occurances of the word in the whole collection of documents\n",
    "# df(word) = occurance of word in # of all documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inverse Document Frequency\n",
    "# inverse of the Document Frequency\n",
    "# measure the informativeness of the word\n",
    "# More occuring words will be very low and give less weighting\n",
    "# We want to weight up words that are unique to the question\n",
    "# idf(word) = # of all documents / df(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TFIDF\n",
    "# Mutiplying Term Frequency and IDF\n",
    "# tf-idf(word,document) = tf(word,document) * log(# of all docuemnts / (df(word) + 1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
