{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # 3. Create a search engine using TFIDF\n",
    "0. Import libraries and dataset\n",
    "1. data preprocess labelled data\n",
    "3. Create TFIDF vectoriser from literatures that are included in the dataset\n",
    "4. Cosine Similarity\n",
    "5. Evaluation of the search engine using the labelled data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.0. Import Libraries and Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\josep\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\josep\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import all the required Library\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "# Text preprocessing libraries\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "nltk.download('wordnet') \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "# libraries for keyword extraction with tf-idf\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the pickle files created from previous notebooks\n",
    "scoped_categorised_literature = pd.read_pickle(\"./1_scoped_cat_lit.pkl\")\n",
    "extracted_literature_data = pd.read_pickle(\"./2_extracted_literature_data.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['extract_id', 'json_path', 'section', 'text'], dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extracted_literature_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Date', 'topic_id', 'research_topic', 'pdf_json_files',\n",
       "       'pmc_json_files', 'Study', 'Study Link', 'Journal', 'Study Type',\n",
       "       'Factors', 'Influential', 'Excerpt', 'Measure of Evidence', 'Added on'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scoped_categorised_literature.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Creating function for text preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'now', 'as', 'hasn', 'the', 'are', 'an', 'all', 'because', 'my', 'he', 'each', \"isn't\", 'there', 'o', 'from', 'why', 'shan', 'during', 'do', 'doing', 'them', 'where', \"aren't\", \"hadn't\", 'other', 'm', \"hasn't\", \"shan't\", 'while', 'had', 'what', 'about', 'mustn', \"she's\", 'at', 'these', 'this', 'y', 'him', 'mightn', \"doesn't\", 'out', 'not', 'wasn', 'me', \"you'll\", 'if', 'to', 'theirs', 'i', 'didn', \"weren't\", 'needn', 'yours', 'with', 'but', 'just', 'most', 'in', 'yourself', 's', 'itself', 'same', 'up', 'once', \"you've\", 'few', \"mustn't\", \"that'll\", 'here', 'then', 'over', 'which', 'can', 've', 'she', 'before', 'above', 'wouldn', 'been', \"couldn't\", \"didn't\", 'couldn', 'be', 'myself', 'no', 'is', 'so', 'after', 'for', 'too', 'they', 're', 'only', \"don't\", 'further', 'nor', \"won't\", 'did', 'or', 'it', 'himself', 'our', 'who', 'against', 'ma', 'was', 'those', 'of', 'how', \"mightn't\", 'should', \"wouldn't\", 'own', 'and', \"you're\", 'hers', 'when', 'some', \"it's\", 'being', 'does', 'doesn', \"needn't\", 'you', 'ain', \"wasn't\", 'until', 'were', 'between', 'on', 'themselves', 'than', \"haven't\", 'll', 'don', 'hadn', \"shouldn't\", 'through', 'herself', 'both', 'more', 'haven', 'down', 'by', 'aren', 'will', 'your', 'their', 'below', 'has', 'we', 'whom', 'a', \"should've\", 'ours', 'ourselves', 'into', 'such', 'isn', 'again', 'her', 'd', 'weren', 'yourselves', 'off', 'that', 'very', 'its', \"you'd\", 'under', 'his', 'shouldn', 'have', 'am', 't', 'having', 'won', 'any'}\n"
     ]
    }
   ],
   "source": [
    "# printing all the stop words\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(inputText):\n",
    "    #define stopwords\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    #lower case the text\n",
    "    outputText = inputText.lower()\n",
    "    #Convrt percentages into the string percent\n",
    "    outputText = re.sub('(\\\\d+%)', 'percent', outputText)\n",
    "    # Remove special characters and digits\n",
    "    outputText=re.sub(\"(\\\\d|\\\\W)+\",\" \",outputText)    \n",
    "    # Tokenisation\n",
    "    outputText = outputText.split()\n",
    "    # Remove Stop Words\n",
    "    outputText = [word for word in outputText if not word in stop_words]\n",
    "    # Stemming\n",
    "    ps=PorterStemmer()\n",
    "    outputText = [ps.stem(word) for word in outputText]\n",
    "    # Lemmatisation\n",
    "    lem = WordNetLemmatizer()\n",
    "    outputText = [lem.lemmatize(word) for word in outputText] \n",
    "    outputText = \" \".join(outputText) \n",
    "    \n",
    "    return outputText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1. Testing the text pre-processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Figure 10 shows that the number of the exposed individuals in region1 decreases from 868.52 (without controls) to 482.05 (with controls) at the end of the implementation of the proposed strategy. Figure 11 demonstrates that the number of the infected individuals in region 1 decreases from 657.01 (without controls) to 364.95 (with controls) at the end of the implementation of the proposed strategy. Also, the number of the quarantined individuals increases significantly from 10.15 (without controls) to 224.57 (with controls).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'figur show number expo individu region decreas without control control end implement propos strategi figur demonstr number infect individu region decreas without control control end implement propos strategi also number quarantin individu increas significantli without control control'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing the preprocessing\n",
    "text = scoped_categorised_literature.iloc[3]['Excerpt']\n",
    "print(text)\n",
    "preprocess(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2. Generate TF-IDF Vector space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying data preprocessing to all the text we've extracted from the JSON file\n",
    "processed_extracted_literature_data = extracted_literature_data['text'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       covid declar pandem date covid affect peopl wo...\n",
      "1       sever acut respiratori syndrom coronaviru sar ...\n",
      "2       label pandem covid affect peopl worldwid major...\n",
      "3       facilit characteris sar cov comparison made be...\n",
      "4       studi look first confirm case ncip provid evid...\n",
      "                              ...                        \n",
      "6491    studi period henc time seri length longer hube...\n",
      "6492    conclus meteorolog factor influenc covid trans...\n",
      "6493    declar conflict interest certifi peer review a...\n",
      "6494    certifi peer review author funder grant medrxi...\n",
      "6495    copyright holder preprint version post march h...\n",
      "Name: text, Length: 6496, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(processed_extracted_literature_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tfidfVectorizer object\n",
    "vectorizer = TfidfVectorizer()\n",
    "#fit_transform method to convert given text into TF-IDF scores for all the documents\n",
    "tfidf_transform = vectorizer.fit_transform(processed_extracted_literature_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Cosine Similarity "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.1. Testing a user query on running the TFIDF search engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 6636)\t0.5407612421224066\n",
      "  (0, 2613)\t0.40779661991205124\n",
      "  (0, 1687)\t0.507304610499241\n",
      "  (0, 739)\t0.5328425921158624\n"
     ]
    }
   ],
   "source": [
    "# Create a test example of how to run the search engine\n",
    "query = 'breathing difficulty give oxygem therapy'\n",
    "query = preprocess(query)\n",
    "query_vec = vectorizer.transform([query])\n",
    "print(query_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    idx  score                                               text\n",
      "0    33  0.382  • Provide supplemental oxygen therapy immediat...\n",
      "1    31  0.259  For patients with severe disease (Figure 6) , ...\n",
      "2   244  0.196  The registered cases continued to increase rap...\n",
      "3  6021  0.162  In this manuscript, a method is presented that...\n",
      "4    34  0.158  • Closely monitor patients with SARI in case o...\n"
     ]
    }
   ],
   "source": [
    "# Using cosine_similarity to get cosine similarities for a query vs all the document available in the text\n",
    "result = cosine_similarity(tfidf_transform, query_vec)\n",
    "result = [i[0] for i in result]\n",
    "\n",
    "# obtaining the top 5 vaules and print the name\n",
    "N = 5\n",
    "top_5_idx = np.argsort(result)[-N:]\n",
    "top_5_idx = top_5_idx.tolist()\n",
    "top_5_idx.reverse()\n",
    "top_5_score = []\n",
    "top_5_text =[]\n",
    "for i in top_5_idx:\n",
    "    top_5_score.append(round(result[i],3))\n",
    "    top_5_text.append(extracted_literature_data.iloc[i]['text'])\n",
    "\n",
    "test_df = pd.DataFrame(zip(top_5_idx, top_5_score, top_5_text), columns = ['idx', 'score', 'text'])\n",
    "print(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4. Evaluation of the search engine using the labelled data\n",
    "The goal is to ask the engine the 11 key questions that researchers are looking for answers to.\n",
    "Based on the top N results from the engine, there should be atleast 1 paragraph that is relevant to what the researcher wants\n",
    "Performance is measured by using the labelled data.\n",
    "\n",
    "1. Using the labelled data, run the paragraphs into the search engine, and get the top 1 paragraph with the best cosin similarity score\n",
    "2. From the step above, we will obtain a list of paragraphs (in search engine index) grouped by each topic questions\n",
    "\n",
    "3. Using the research topic itself, query the search engine and get top N result (i.e. 5).\n",
    "\n",
    "4. check whether or not the top N result matches with the list of paragraphs that were obtained from the labelled data\n",
    "\n",
    "5. only one match needs to occur per query to get a correct response\n",
    "\n",
    "6. divide the correct response by the total amount of query to get the accuracy of the search engine model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.1. Obtaining the list of key research topics with their topic ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    topic_id                                     research_topic\n",
      "1          1  Effectiveness of a multifactorial strategy to ...\n",
      "0          2  Effectiveness of case isolation_isolation of e...\n",
      "0          3       Effectiveness of community contact reduction\n",
      "0          4    Effectiveness of inter_inner travel restriction\n",
      "0          5                 Effectiveness of school distancing\n",
      "1          6  Effectiveness of workplace distancing to preve...\n",
      "10         7  Evidence that domesticated_farm animals can be...\n",
      "0          8  How does temperature and humidity affect the t...\n",
      "0          9  Methods to understand and regulate the spread ...\n",
      "0         10                        Seasonality of transmission\n",
      "0         11  What is the likelihood of significant changes ...\n"
     ]
    }
   ],
   "source": [
    "# Using the labelled dataset\n",
    "topic_list = scoped_categorised_literature[['topic_id', 'research_topic']].drop_duplicates()\n",
    "print(topic_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.2. Find the top match for the labelled data through the search engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_id_list = []\n",
    "excerpt_list = []\n",
    "top_idx_list = []\n",
    "top_score_list = []\n",
    "top_text_list = []\n",
    "for index, row in scoped_categorised_literature.iterrows():\n",
    "    topic_id_list.append(row['topic_id'])\n",
    "    excerpt_list.append(row['Excerpt'])\n",
    "    \n",
    "    query = preprocess(row['Excerpt'])\n",
    "    query = vectorizer.transform([query])\n",
    "    cos_result = cosine_similarity(tfidf_transform, query)\n",
    "    cos_result = [i[0] for i in cos_result]\n",
    "    top_idx = np.argsort(cos_result)[-1]\n",
    "    top_score = round(cos_result[top_idx],3)\n",
    "    \n",
    "    top_idx_list.append(top_idx)\n",
    "    top_score_list.append(top_score)\n",
    "    top_text_list.append(extracted_literature_data.iloc[top_idx]['text'])\n",
    "\n",
    "labelled_topic_output = pd.DataFrame(zip(topic_id_list,excerpt_list,top_idx_list,top_score_list,top_text_list), \n",
    "                                     columns = ['topic_id', 'exceprt', 'top_idx', 'top_score', 'top_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic_id</th>\n",
       "      <th>exceprt</th>\n",
       "      <th>top_idx</th>\n",
       "      <th>top_score</th>\n",
       "      <th>top_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Comparing these four scenarios, we shall deduc...</td>\n",
       "      <td>208</td>\n",
       "      <td>0.638</td>\n",
       "      <td>Par. Scenarios B 1 and B 2 show cases in which...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Our study reveals that the strict control meas...</td>\n",
       "      <td>4569</td>\n",
       "      <td>0.571</td>\n",
       "      <td>Background: The ongoing COVID-19 epidemic dila...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>We then compare the transmission rates in diff...</td>\n",
       "      <td>5021</td>\n",
       "      <td>0.799</td>\n",
       "      <td>more cases within a week, implying a fast grow...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Figure 10 shows that the number of the exposed...</td>\n",
       "      <td>283</td>\n",
       "      <td>0.827</td>\n",
       "      <td>To realize this strategy, we apply only the co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Lockdown showed highest reduction (28%) in num...</td>\n",
       "      <td>1515</td>\n",
       "      <td>0.643</td>\n",
       "      <td>The copyright holder for this preprint this ve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>11</td>\n",
       "      <td>Generally, the curves tended to be not associa...</td>\n",
       "      <td>4073</td>\n",
       "      <td>0.898</td>\n",
       "      <td>is the (which was not peer-reviewed) The copyr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>11</td>\n",
       "      <td>We find the high temperature and relative humi...</td>\n",
       "      <td>2975</td>\n",
       "      <td>0.657</td>\n",
       "      <td>We find that temperature negatively relates to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>11</td>\n",
       "      <td>We find the high temperature and relative humi...</td>\n",
       "      <td>2975</td>\n",
       "      <td>0.657</td>\n",
       "      <td>We find that temperature negatively relates to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>11</td>\n",
       "      <td>The regression model, demonstrates that both a...</td>\n",
       "      <td>6425</td>\n",
       "      <td>0.948</td>\n",
       "      <td>Relationship with environmental factors. The r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>11</td>\n",
       "      <td>The regression model, demonstrates that both a...</td>\n",
       "      <td>6425</td>\n",
       "      <td>0.948</td>\n",
       "      <td>Relationship with environmental factors. The r...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     topic_id                                            exceprt  top_idx  \\\n",
       "0           1  Comparing these four scenarios, we shall deduc...      208   \n",
       "1           1  Our study reveals that the strict control meas...     4569   \n",
       "2           1  We then compare the transmission rates in diff...     5021   \n",
       "3           1  Figure 10 shows that the number of the exposed...      283   \n",
       "4           1  Lockdown showed highest reduction (28%) in num...     1515   \n",
       "..        ...                                                ...      ...   \n",
       "395        11  Generally, the curves tended to be not associa...     4073   \n",
       "396        11  We find the high temperature and relative humi...     2975   \n",
       "397        11  We find the high temperature and relative humi...     2975   \n",
       "398        11  The regression model, demonstrates that both a...     6425   \n",
       "399        11  The regression model, demonstrates that both a...     6425   \n",
       "\n",
       "     top_score                                           top_text  \n",
       "0        0.638  Par. Scenarios B 1 and B 2 show cases in which...  \n",
       "1        0.571  Background: The ongoing COVID-19 epidemic dila...  \n",
       "2        0.799  more cases within a week, implying a fast grow...  \n",
       "3        0.827  To realize this strategy, we apply only the co...  \n",
       "4        0.643  The copyright holder for this preprint this ve...  \n",
       "..         ...                                                ...  \n",
       "395      0.898  is the (which was not peer-reviewed) The copyr...  \n",
       "396      0.657  We find that temperature negatively relates to...  \n",
       "397      0.657  We find that temperature negatively relates to...  \n",
       "398      0.948  Relationship with environmental factors. The r...  \n",
       "399      0.948  Relationship with environmental factors. The r...  \n",
       "\n",
       "[400 rows x 5 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelled_topic_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The epidemic would ultimately infect approximately 77% of the population (Fig 2B) and result in around 350 thousand fatalities among individuals aged over 60, and around 60 thousand aged below 60 (Fig 2C). Sustained social-distancing by older individuals (assumed to result in a 90% reduction in contacts with individuals under 25, a 70% reduction with 25-59, and a 50% reduction between one another), and moderately effective self-isolation by symptomatic individuals (at 20% efficacy) results in a shallower epidemic curve (Fig 2D) and a much smaller outbreak size among individuals aged 60+\n",
      "____________________________________________\n",
      "Sustained social-distancing by older individuals (assumed to result in a 90% reduction in contacts with individuals under 25, a 70% reduction with 25-59, and a 50% reduction between one another), and moderately effective self-isolation by symptomatic individuals (at 20% efficacy) results in a shallower epidemic curve ( Fig 2D) and a much smaller outbreak size among individuals aged 60+ (Fig 2E) . The attendant mortality burden among 60+ individuals is also substantially reduced (to 62 thousand), with a smaller reduction in fatalities in those aged < 60 (to 43 thousand; Fig 2F) .\n"
     ]
    }
   ],
   "source": [
    "#Check whether the input text matches with the paragraphs output from the search engine\n",
    "x = 5\n",
    "print(labelled_topic_output.iloc[x]['exceprt'])\n",
    "print('____________________________________________')\n",
    "print(labelled_topic_output.iloc[x]['top_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.3. Using the research topic itself, query the search engine and get top 5 results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 289, 290, 285, 27, 291]\n",
      "[2, 4674, 5846, 2375, 291, 4686]\n",
      "[3, 3403, 5180, 3379, 3862, 2127]\n",
      "[4, 4745, 1036, 4248, 1006, 2963]\n",
      "[5, 4346, 3405, 4035, 1232, 4378]\n",
      "[6, 6380, 6401, 446, 6369, 6408]\n",
      "[7, 259, 254, 3438, 6137, 249]\n",
      "[8, 2975, 4922, 3021, 4883, 3072]\n",
      "[9, 2578, 3096, 3628, 2064, 3074]\n",
      "[10, 3727, 791, 2135, 5586, 2133]\n",
      "[11, 5262, 2320, 5077, 5744, 5254]\n"
     ]
    }
   ],
   "source": [
    "# now query the search engine based on the scientific question\n",
    "topic_question_top_5_result_list = []\n",
    "\n",
    "for index, row in topic_list.iterrows():  \n",
    "    query = preprocess(row['research_topic'])\n",
    "    query = vectorizer.transform([query])    \n",
    "    result = cosine_similarity(tfidf_transform, query)\n",
    "    result = [i[0] for i in result]\n",
    "    # obtaining the top 5 vaules and print the name\n",
    "    N = 5\n",
    "    top_5_idx = np.argsort(result)[-N:]\n",
    "    top_5_idx = top_5_idx.tolist()\n",
    "    top_5_idx.reverse()\n",
    "    \n",
    "    output = [row['topic_id']] + top_5_idx\n",
    "    topic_question_top_5_result_list.append(output)\n",
    "\n",
    "for topic_question_top_5_result in topic_question_top_5_result_list:\n",
    "    print(topic_question_top_5_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.4. Evaulate the accuracy of the Search engine\n",
    "1. Check whether the top 5 result from the key topics query matches with the indexes obtained from the labelled data\n",
    "2. Only one match needs to occur per query to get a correct response\n",
    "3. divide the correct response by the total amount of query to get the accuracy of the search engine model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the Search Engine based on the above criteria is:  2 / 11 = 0.182\n"
     ]
    }
   ],
   "source": [
    "match_count = 0\n",
    "questions_asked = 0\n",
    "for topic_question_top_5_result in topic_question_top_5_result_list:\n",
    "    match = False    \n",
    "    for i in range(1,len(topic_question_top_5_result)):\n",
    "        topic_id = topic_question_top_5_result[0]\n",
    "        labelled_topic_id_df = labelled_topic_output[labelled_topic_output['topic_id'] == topic_id]\n",
    "        if not labelled_topic_id_df[labelled_topic_id_df['top_idx'] == topic_question_top_5_result[i]].empty:\n",
    "            match = True\n",
    "    if match == True:\n",
    "        match_count = match_count + 1\n",
    "    questions_asked = questions_asked + 1\n",
    "\n",
    "print(\"The accuracy of the Search Engine based on the above criteria is: \", match_count, \"/\", questions_asked, \"=\", round(match_count / questions_asked,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
